{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2: DDI using ML\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.corpus import stopwords \n",
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "devel_path = '../../data/Devel'\n",
    "test_path = '../../data/Test-DDI'\n",
    "train_path = '../../data/Train'\n",
    "\n",
    "model_path = '/mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/model/'\n",
    "output_path = '/mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/output/'\n",
    "features_path = '/mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/features/'\n",
    "\n",
    "my_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_symbols(node):\n",
    "    if node['word'] == '-LRB-':\n",
    "        node['word'] = '('\n",
    "        node['lemma'] = '('\n",
    "    elif node['word'] == '-RRB-':\n",
    "        node['word'] = ')'\n",
    "        node['lemma'] = ')'\n",
    "    elif node['word'] == '-LSB-':\n",
    "        node['word'] = '['\n",
    "        node['lemma'] = '['\n",
    "    elif node['word'] == '-RSB-':\n",
    "        node['word'] = ']'\n",
    "        node['lemma'] = ']'\n",
    "    elif node['word'] in [\"``\", \"''\"]:\n",
    "        node['word'] = '\"'\n",
    "        node['lemma'] = '\"'\n",
    "    return\n",
    "    \n",
    "def analyze(sent):\n",
    "    if len(sent)<= 0:\n",
    "        return None\n",
    "    \n",
    "    mytree, = my_parser.raw_parse(sent)\n",
    "    tree = mytree.nodes\n",
    "    ini_token = 0\n",
    "                   \n",
    "    # clean tree\n",
    "    info = ['address', 'head', 'lemma', 'rel', 'word', 'tag']\n",
    "    \n",
    "    for k in sorted(tree.keys()):\n",
    "        node = tree[k] \n",
    "        for key in list(node):\n",
    "            if key not in info:\n",
    "                del node[key]\n",
    "        \n",
    "        handle_special_symbols(node)\n",
    "        \n",
    "        if k != 0:\n",
    "            # add offsets\n",
    "            ini_token = sent.find(node['word'] ,ini_token)\n",
    "            node['start'] = ini_token\n",
    "            ini_token += len(node['word'])\n",
    "            node['end'] = ini_token - 1\n",
    "            \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_nodes(tree, entities, e1, e2):\n",
    "    entity1 = []\n",
    "    entity2 = []\n",
    "\n",
    "    starts1 = [offs[0] for offs in entities[e1]]\n",
    "    starts2 = [offs[0] for offs in entities[e2]]\n",
    "    ends1 = [offs[1] for offs in entities[e1]]\n",
    "    ends2 = [offs[1] for offs in entities[e2]]\n",
    "    \n",
    "    for k in sorted(tree.keys()):\n",
    "        if 'start' in tree[k].keys():\n",
    "            for i in range(len(starts1)):\n",
    "                if int(starts1[i]) in range(tree[k]['start'], tree[k]['end']+1) or int(ends1[i]) in range(tree[k]['start'], tree[k]['end']+1):\n",
    "                    entity1.append(tree[k])\n",
    "                elif tree[k]['start'] in range(int(starts1[i]), int(ends1[i])+1) and tree[k]['end'] in range(int(starts1[i]), int(ends1[i])+1):\n",
    "                    entity1.append(tree[k])\n",
    "                    \n",
    "            for i in range(len(starts2)):\n",
    "                if int(starts2[i]) in range(tree[k]['start'], tree[k]['end']+1) or int(ends2[i]) in range(tree[k]['start'], tree[k]['end']+1):\n",
    "                    entity2.append(tree[k])\n",
    "                elif tree[k]['start'] in range(int(starts2[i]), int(ends2[i])+1) and tree[k]['end'] in range(int(starts2[i]), int(ends2[i])+1):\n",
    "                    entity2.append(tree[k])\n",
    "                    \n",
    "    return entity1, entity2\n",
    "\n",
    "\n",
    "def is_under(entity1, entity2):\n",
    "    for i in range(len(entity1)):\n",
    "        if entity1[i]['head'] in [e['address'] for e in entity2]:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def get_entity_parent(tree, entity):\n",
    "    if len(entity) == 1:\n",
    "        return tree[entity[0]['head']], entity[0]['rel']\n",
    "    else:\n",
    "        parent = None\n",
    "        rel = None\n",
    "        for e in entity:\n",
    "            if e['head'] not in [other['address'] for other in entity]:\n",
    "                parent = tree[e['head']]\n",
    "                rel = e['rel']\n",
    "        return parent, rel\n",
    "    \n",
    "def same(parent1, parent2):\n",
    "    if parent1['address'] == parent2['address']:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def parent_lemma_belongs(parent, lemma_set):\n",
    "    if parent['lemma'] in lemma_set:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def get_entity_limits(entity):\n",
    "    start = min([e['start'] for e in entity])\n",
    "    end = max([e['end'] for e in entity])\n",
    "    return start, end\n",
    "    \n",
    "    \n",
    "def extract_features_1(tree, entities, e1, e2):\n",
    "    # Get entities\n",
    "    entity1, entity2 = get_entity_nodes(tree, entities, e1, e2)\n",
    "    \n",
    "    # Features\n",
    "    features = []\n",
    "    \n",
    "    parent1, rel1 = get_entity_parent(tree, entity1)\n",
    "    parent2, rel2 = get_entity_parent(tree, entity2)\n",
    "    \n",
    "    features.extend(['h1_lemma=%s' % parent1['lemma'],\n",
    "                     'h1_word=%s' % parent1['word'],\n",
    "                     'h1_tag=%s' % parent1['tag'],\n",
    "                     'h1_rel=%s' % rel1,\n",
    "                     'h2_lemma=%s' % parent2['lemma'],\n",
    "                     'h2_word=%s' % parent2['word'],\n",
    "                     'h2_tag=%s' % parent2['tag'],\n",
    "                     'h2_rel=%s' % rel2,\n",
    "                    ])\n",
    "    \n",
    "    if same(parent1, parent2):\n",
    "        features.append('under_same')\n",
    "        if parent1['tag'][0].lower() == 'v':\n",
    "            features.append('under_same_verb')\n",
    "    \n",
    "    if is_under(entity1, entity2):\n",
    "        features.append('1under2')\n",
    "    \n",
    "    if is_under(entity2, entity1):\n",
    "        features.append('2under1')\n",
    "        \n",
    "    if parent_lemma_belongs(parent1, ['interact', 'interaction']):\n",
    "        features.append('interaction')\n",
    "    \n",
    "    start1, end1 = get_entity_limits(entity1)\n",
    "    start2, end2 = get_entity_limits(entity2)\n",
    "\n",
    "    for k in sorted(tree.keys()):\n",
    "        if 'start' in tree[k].keys() and  tree[k]['start'] < start1:\n",
    "            features.append('lb1=%s' % tree[k]['lemma'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end1 < tree[k]['start'] < start2:\n",
    "            features.append('lib=%s' % tree[k]['lemma'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end2 < tree[k]['start']:\n",
    "            features.append('la2=%s' % tree[k]['lemma'])\n",
    "      \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_parent_v2(tree, entity):\n",
    "    if len(entity) == 1:\n",
    "        return [tree[entity[0]['head']]], [entity[0]['rel']]\n",
    "    else:\n",
    "        parent = []\n",
    "        rel = []\n",
    "        for e in entity:\n",
    "            if e['head'] not in [other['address'] for other in entity]:\n",
    "                parent.append(tree[e['head']])\n",
    "                rel.append(e['rel'])\n",
    "        return parent, rel\n",
    "    \n",
    "def same_v2(parent1, parent2):\n",
    "    for p1 in parent1:\n",
    "        for p2 in parent2:\n",
    "            if p1['address'] == p2['address']:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def same_verb_v2(parent1, parent2):\n",
    "    for p1 in parent1:\n",
    "        for p2 in parent2:\n",
    "            if p1['address'] == p2['address'] and p1['tag'][0].lower() == 'v':\n",
    "                return True\n",
    "    return False\n",
    "    \n",
    "def parent_lemma_belongs_v2(parent, lemma_set):\n",
    "    for p in parent:\n",
    "        if p['lemma'] in lemma_set:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def extract_features_2(tree, entities, e1, e2):\n",
    "    # Get entities\n",
    "    entity1, entity2 = get_entity_nodes(tree, entities, e1, e2)\n",
    "    \n",
    "    # Features\n",
    "    features = []\n",
    "    \n",
    "    parent1, rel1 = get_entity_parent_v2(tree, entity1)\n",
    "    parent2, rel2 = get_entity_parent_v2(tree, entity2)\n",
    "    \n",
    "    for i in range(len(parent1)):\n",
    "        features.extend(['h1_lemma_%d=%s' % (i, parent1[i]['lemma']),\n",
    "                         'h1_word_%d=%s' % (i, parent1[i]['word']),\n",
    "                         'h1_tag_%d=%s' % (i, parent1[i]['tag']),\n",
    "                         'h1_rel_%d=%s' % (i, rel1[i])])\n",
    "    for i in range(len(parent2)):\n",
    "        features.extend(['h2_lemma_%d=%s' % (i, parent2[i]['lemma']),\n",
    "                         'h2_word_%d=%s' % (i, parent2[i]['word']),\n",
    "                         'h2_tag_%d=%s' % (i, parent2[i]['tag']),\n",
    "                         'h2_rel_%d=%s' % (i, rel2[i])])\n",
    "    \n",
    "    if same_v2(parent1, parent2):\n",
    "        features.append('under_same')\n",
    "    \n",
    "    if same_verb_v2(parent1, parent2):\n",
    "        features.append('under_same_verb')\n",
    "    \n",
    "    if is_under(entity1, entity2):\n",
    "        features.append('1under2')\n",
    "    \n",
    "    if is_under(entity2, entity1):\n",
    "        features.append('2under1')\n",
    "        \n",
    "    if parent_lemma_belongs_v2(parent1, ['interact', 'interaction']):\n",
    "        features.append('interaction')\n",
    "    \n",
    "    start1, end1 = get_entity_limits(entity1)\n",
    "    start2, end2 = get_entity_limits(entity2)\n",
    "\n",
    "    for k in sorted(tree.keys()):\n",
    "        if 'start' in tree[k].keys() and  tree[k]['start'] < start1:\n",
    "            features.append('lb1=%s' % tree[k]['lemma'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end1 < tree[k]['start'] < start2:\n",
    "            features.append('lib=%s' % tree[k]['lemma'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end2 < tree[k]['start']:\n",
    "            features.append('la2=%s' % tree[k]['lemma'])\n",
    "      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_under_v2(entity1, entity2):\n",
    "    for i in range(len(entity1)):\n",
    "        if entity1[i]['head'] in [e['address'] for e in entity2]:\n",
    "            return True, entity1[i]['rel']\n",
    "    return False, None\n",
    "\n",
    "def extract_features_3(tree, entities, e1, e2):\n",
    "    # Get entities\n",
    "    entity1, entity2 = get_entity_nodes(tree, entities, e1, e2)\n",
    "    \n",
    "    # Features\n",
    "    features = []\n",
    "    \n",
    "    parent1, rel1 = get_entity_parent_v2(tree, entity1)\n",
    "    parent2, rel2 = get_entity_parent_v2(tree, entity2)\n",
    "    \n",
    "    for i in range(len(parent1)):\n",
    "        features.extend(['h1_lemma_%d=%s' % (i, parent1[i]['lemma']),\n",
    "                         'h1_word_%d=%s' % (i, parent1[i]['word']),\n",
    "                         'h1_tag_%d=%s' % (i, parent1[i]['tag']),\n",
    "                         'h1_rel_%d=%s' % (i, rel1[i])])\n",
    "    for i in range(len(parent2)):\n",
    "        features.extend(['h2_lemma_%d=%s' % (i, parent2[i]['lemma']),\n",
    "                         'h2_word_%d=%s' % (i, parent2[i]['word']),\n",
    "                         'h2_tag_%d=%s' % (i, parent2[i]['tag']),\n",
    "                         'h2_rel_%d=%s' % (i, rel2[i])])\n",
    "    \n",
    "    if same_v2(parent1, parent2):\n",
    "        features.append('under_same')\n",
    "    \n",
    "    if same_verb_v2(parent1, parent2):\n",
    "        features.append('under_same_verb')\n",
    "        \n",
    "    is_1_under_2, rel_1_2 = is_under_v2(entity1, entity2)\n",
    "    if is_1_under_2:\n",
    "        features.append('1under2')\n",
    "        features.append('1under2_rel=%s' % rel_1_2)\n",
    "    \n",
    "    is_2_under_1, rel_2_1 = is_under_v2(entity2, entity1)\n",
    "    if is_2_under_1:\n",
    "        features.append('2under1')\n",
    "        features.append('2under1_rel=%s' % rel_2_1)\n",
    "        \n",
    "    if parent_lemma_belongs_v2(parent1, ['interact', 'interaction']):\n",
    "        features.append('interaction')\n",
    "    \n",
    "    start1, end1 = get_entity_limits(entity1)\n",
    "    start2, end2 = get_entity_limits(entity2)\n",
    "\n",
    "    for k in sorted(tree.keys()):\n",
    "        if 'start' in tree[k].keys() and  tree[k]['start'] < start1:\n",
    "            features.append('lb1=%s' % tree[k]['lemma'])\n",
    "            features.append('pb1=%s' % tree[k]['tag'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end1 < tree[k]['start'] < start2:\n",
    "            features.append('lib=%s' % tree[k]['lemma'])\n",
    "            features.append('pib=%s' % tree[k]['tag'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end2 < tree[k]['start']:\n",
    "            features.append('la2=%s' % tree[k]['lemma'])\n",
    "            features.append('pa2=%s' % tree[k]['tag'])\n",
    "      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_features(sent_id, e1, e2, gold_class, features, extra_info_outf, outf):\n",
    "    features_str = '\\t'.join(features)\n",
    "    outf.write(gold_class+'\\t'+features_str)\n",
    "    outf.write(\"\\n\")\n",
    "    extra_info_outf.write(sent_id+'\\t'+e1+'\\t'+e2+'\\t'+gold_class+'\\t'+features_str)\n",
    "    extra_info_outf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_extract_features(input_dir, extra_info_output_file, output_file, number):\n",
    "    outf = open(output_file, \"w\")\n",
    "    extra_info_outf = open(extra_info_output_file, \"w\")\n",
    "    # process each file in directory\n",
    "    for filename in os.listdir(input_dir):  \n",
    "        # parse XML file, obtaining a DOM tree\n",
    "        fullname = os.path.join(input_dir, filename)\n",
    "        tree = ET.parse(fullname)\n",
    "        root = tree.getroot()    \n",
    "        \n",
    "        # process each sentence in the file\n",
    "        for sentence in root.findall('sentence'):\n",
    "            # Get sentence id and tokenize text\n",
    "            sent_id = sentence.get('id') # get sentence id\n",
    "            sent_text = sentence.get('text') #get sentence text\n",
    "            splitted_sent = sent_text.split('\\r\\n')\n",
    "            sent_text = splitted_sent[0]\n",
    "            for s in splitted_sent[1:]:\n",
    "                if len(s) >= 1:\n",
    "                    sent_text += ('. ' + s)\n",
    "            \n",
    "            # load sentence entities into a dictionary\n",
    "            entities = {}\n",
    "            for ent in sentence.findall('entity'):\n",
    "                ent_id = ent.get('id')\n",
    "                offsets = ent.get('charOffset').split(';')\n",
    "                offs = [o.split('-') for o in offsets]\n",
    "                entities[ent_id] = offs\n",
    "            \n",
    "            tree = analyze(sent_text)\n",
    "            # for each pair in the sentence, decide whether it is DDI and its type\n",
    "            for pair in sentence.findall('pair'):\n",
    "                e1 = pair.get('e1')\n",
    "                e2 = pair.get('e2')\n",
    "                \n",
    "                gold_class = pair.get('type')\n",
    "                if gold_class is None:\n",
    "                    gold_class = 'null'\n",
    "                \n",
    "                if number == 1:\n",
    "                    features = extract_features_1(tree, entities, e1, e2)\n",
    "                elif number == 2:\n",
    "                    features = extract_features_2(tree, entities, e1, e2)\n",
    "                else:\n",
    "                    features = extract_features_3(tree, entities, e1, e2)\n",
    "                output_features(sent_id, e1, e2, gold_class, features, extra_info_outf, outf)\n",
    "    outf.close()\n",
    "    extra_info_outf.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_extract_features(train_path, 'features/info_train_features_1', 'features/train_features_1', 1)\n",
    "main_extract_features(devel_path, 'features/info_devel_features_1', 'features/devel_features_1', 1)\n",
    "main_extract_features(test_path, 'features/info_test_features_1', 'features/test_features_1', 1)\n",
    "\n",
    "main_extract_features(train_path, 'features/info_train_features_2', 'features/train_features_2', 2)\n",
    "main_extract_features(devel_path, 'features/info_devel_features_2', 'features/devel_features_2', 2)\n",
    "main_extract_features(test_path, 'features/info_test_features_2', 'features/test_features_2', 2)\n",
    "\n",
    "main_extract_features(train_path, 'features/info_train_features_3', 'features/train_features_3', 3)\n",
    "main_extract_features(devel_path, 'features/info_devel_features_3', 'features/devel_features_3', 3)\n",
    "main_extract_features(test_path, 'features/info_test_features_3', 'features/test_features_3', 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MaxEntropy Classifier\n",
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(megam, parameters, features_file, out_train_model):\n",
    "    print(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" multiclass \" + features_file + \" > \" + out_train_model + \"\\\"\")\n",
    "    os.system(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" multiclass \" + features_file + \" > \" + out_train_model + \"\\\"\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu run \"/mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/model/megam-64.opt -quiet -nc -nobias -repeat 5  -tune -lambda 0.01  -minfc 3 multiclass /mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/features/train_features_3 > /mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/model/model.dat\"\n"
     ]
    }
   ],
   "source": [
    "train(model_path+'megam-64.opt', '-quiet -nc -nobias -repeat 5  -tune -lambda 0.01  -minfc 3', features_path+'train_features_3', model_path+'model.dat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_1 = ['-nobias', '']\n",
    "# param_2 = ['-repeat 5', '-repeat 10', '']\n",
    "# param_3 = ['-norm2', '']\n",
    "# param_4 = ['-tune', '']\n",
    "# param_5 = ['-lambda 0.9', '-lambda 0.01', '']\n",
    "# param_6 = ['']\n",
    "# param_7 = ['-minfc 3', '-minfc 5', '']\n",
    "\n",
    "# param_1 = ['']\n",
    "# param_2 = ['-repeat 5', '-repeat 10']\n",
    "# param_3 = ['']\n",
    "# param_4 = ['-tune', '']\n",
    "# param_5 = ['-lambda 0.9', '-lambda 0.01', '']\n",
    "# param_6 = ['']\n",
    "# param_7 = ['-minfc 1', '-minfc 3', '']\n",
    "\n",
    "# param_1 = ['-nobias']\n",
    "# param_2 = ['-repeat 10']\n",
    "# param_3 = ['-norm2']\n",
    "# param_4 = ['-tune']\n",
    "# param_5 = ['-lambda 0.9', '-lambda 0.01', '']\n",
    "# param_6 = ['']\n",
    "# param_7 = ['-minfc 3', '-minfc 5', '']\n",
    "\n",
    "# #' -quiet -nc -nobias -repeat 10 -norm2 -tune -minfc 3 '\n",
    "\n",
    "# all_parameters = []\n",
    "# model_nb = 0\n",
    "# for p1 in param_1:\n",
    "#     for p2 in param_2:\n",
    "#         for p3 in param_3:\n",
    "#             for p4 in param_4:\n",
    "#                 for p5 in param_5:\n",
    "#                     for p6 in param_6:\n",
    "#                         for p7 in param_7:\n",
    "#                             parameters = ' '.join(['-quiet -nc', p1, p2, p3, p4, p5, p6, p7])\n",
    "#                             print(parameters)\n",
    "#                             for feat in ['1', '2', '3']:    \n",
    "#                                 train(model_path+'megam-64.opt', parameters, features_path+'train_features_'+feat, model_path+'model_'+str(model_nb)+'_f'+feat+'.dat')\n",
    "#                                 outf = open('model/parameters_'+str(model_nb)+'_f'+feat, \"w\")\n",
    "#                                 outf.write(parameters)\n",
    "#                                 outf.close()\n",
    "#                             all_parameters.append(parameters)\n",
    "#                             print(\"Model nb:\", model_nb)\n",
    "#                             model_nb += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(megam, parameters, features_file, train_model, prediction_output):\n",
    "    print(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" -predict \" + train_model + \" multiclass \" + features_file + \" > \" + prediction_output + \"\\\"\")\n",
    "    os.system(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" -predict \" + train_model + \" multiclass \" + features_file + \" > \" + prediction_output + \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu run \"/mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/model/megam-64.opt -quiet -nc -nobias -predict /mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/model/model.dat multiclass /mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/features/devel_features_3 > /mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/output/devel_prediction_v2\"\n"
     ]
    }
   ],
   "source": [
    "# Devel prediction\n",
    "classify(model_path+'megam-64.opt', '-quiet -nc -nobias', features_path+'devel_features_3', model_path+'model.dat', output_path+'devel_prediction_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ubuntu run \"/mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/model/megam-64.opt -quiet -nc -nobias -predict /mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/model/model.dat multiclass /mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/features/test_features_3 > /mnt/d/Users/Albert/OneDrive/Educació/Master\\ in\\ Artificial\\ Intelligence\\ \\(UPC\\)/Semester\\ 2/AHLT/Lab/AHLT-MAI/source/ddi/output/test_prediction_v2\"\n"
     ]
    }
   ],
   "source": [
    "# Test prediction\n",
    "classify(model_path+'megam-64.opt', '-quiet -nc -nobias', features_path+'test_features_3', model_path+'model.dat', output_path+'test_prediction_v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(all_parameters)):\n",
    "#     for feat in ['1', '2', '3']:\n",
    "#         classify(model_path+'megam-64.opt', all_parameters[i], features_path+'devel_features_'+feat, model_path+'model_'+str(i)+'_f'+feat+'.dat', output_path+'devel_prediction_'+str(i)+'_f'+feat+'.dat')\n",
    "#         classify(model_path+'megam-64.opt', all_parameters[i], features_path+'test_features_'+feat, model_path+'model_'+str(i)+'_f'+feat+'.dat', output_path+'test_prediction_'+str(i)+'_f'+feat+'.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce output and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_ddi(sent_id, id_e1, id_e2, predicted_class, outf):\n",
    "    if predicted_class == 'null':\n",
    "        outf.write(sent_id+'|'+id_e1+'|'+id_e2+'|0|'+predicted_class)\n",
    "    else:\n",
    "        outf.write(sent_id+'|'+id_e1+'|'+id_e2+'|1|'+predicted_class)\n",
    "    outf.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_dir, output_file):\n",
    "    os.system(\"java -jar ../../eval/evaluateDDI.jar \"+ str(input_dir) + \" \" + str(output_file))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_function(input_dir, features_info, prediction_file, output_file):\n",
    "    outf = open(output_file, \"w\")\n",
    "    \n",
    "    with open(features_info) as textfile1, open(prediction_file) as textfile2: \n",
    "        for x, y in zip(textfile1, textfile2):\n",
    "            saved_features = x.split('\\t')\n",
    "            sent_id = saved_features[0]\n",
    "            id_e1 = saved_features[1]\n",
    "            id_e2 = saved_features[2]\n",
    "            \n",
    "            predicted_class = y.split()[0]\n",
    "            \n",
    "            output_ddi(sent_id, id_e1, id_e2, predicted_class, outf)\n",
    "    outf.close()\n",
    "    # get performance score\n",
    "    evaluate(input_dir, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_function(devel_path, 'features/info_devel_features', 'output/devel_prediction', 'output/task9.2_develGoal_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_function(test_path, 'features/info_test_features', 'output/test_prediction', 'output/task9.2_testGoal_1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(all_parameters)):\n",
    "    for feat in ['1', '2', '3']:\n",
    "        main_function(devel_path, 'features/info_devel_features_'+feat, 'output/devel_prediction_'+str(i)+'_f'+feat+'.dat', 'output/task9.2_develGoal'+str(i)+'_f'+feat+'.txt')\n",
    "        main_function(test_path, 'features/info_test_features_'+feat, 'output/test_prediction_'+str(i)+'_f'+feat+'.dat', 'output/task9.2_testGoal'+str(i)+'_f'+feat+'.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
