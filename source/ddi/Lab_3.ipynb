{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3: DDI \n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.corpus import stopwords \n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputdir = '../../data/Devel'\n",
    "test_path = '../../data/Test-DDI'\n",
    "train_path = '../../data/Train'\n",
    "outputfile = 'task9.2_develGoal_1.txt'\n",
    "\n",
    "my_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(sent):\n",
    "    if len(sent)<= 0:\n",
    "        return None\n",
    "    mytree, = my_parser.raw_parse(sent)\n",
    "    tree = mytree.nodes\n",
    "    ini_token = 0\n",
    "                   \n",
    "    # clean tree\n",
    "    aux = ['address', 'head', 'lemma', 'rel', 'word', 'tag']\n",
    "    for k in range(1,len(tree)):\n",
    "        node = tree[k] \n",
    "        for key in list(node):\n",
    "            if key not in aux:\n",
    "                del node[key]\n",
    "        \n",
    "        # add offset\n",
    "        ini_token = sent.find(node['word'] ,ini_token)\n",
    "        \n",
    "        node['start'] = ini_token\n",
    "        ini_token += len(node['word'])\n",
    "        node['end'] = ini_token\n",
    "    return tree\n",
    "\n",
    "def check_interaction(analysis, entities, e1, e2):\n",
    "    \n",
    "    # In between words per interaction\n",
    "    type_effect = ['administer', 'potentiate', 'prevent']\n",
    "    type_mechanism = ['reduce', 'increase', 'decrease']\n",
    "    type_int = ['interact', 'interaction']\n",
    "    type_advise = []\n",
    "    \n",
    "    # Rules\n",
    "    \n",
    "    # get words between the entities\n",
    "    flag = 0\n",
    "    between_words = []\n",
    "    for key in analysis.keys():\n",
    "        if analysis[key][\"word\"] == e1 or analysis[key][\"word\"] == e2 : flag = !falg\n",
    "        if flag: between_words.append(analysis[key][\"word\"])\n",
    "            \n",
    "    \n",
    "    if len(between_words) > 0:\n",
    "        if len(list(set(type_effect) & set(between_words)))> 0:\n",
    "            return (1,\"effect\")\n",
    "        elif len(list(set(type_mechanism) & set(between_words)))> 0:\n",
    "            return (1,\"mechanism\")\n",
    "        elif len(list(set(type_int) & set(between_words)))> 0:\n",
    "            return (1,\"int\")\n",
    "        elif len(list(set(type_advise) & set(between_words)))> 0:\n",
    "            return (1,\"advise\")\n",
    "    else: return (0, \"null\")\n",
    "    \n",
    "\n",
    "def evaluate(inputdir, outputfile):\n",
    "    os.system(\"java -jar eval/evaluateDDI.jar \"+ str(inputdir) + \" \" + str(outputfile))\n",
    "    return \n",
    "\n",
    "def main_function(inputdir):\n",
    "    # process each file in directory\n",
    "    for filename in os.listdir(inputdir):\n",
    "        # parse XML file, obtaining a DOM tree\n",
    "        fullname = os.path.join(inputdir, filename)\n",
    "        tree = ET.parse(fullname)\n",
    "        root = tree.getroot()  \n",
    "        \n",
    "        # process each sentence in the file\n",
    "        for sentence in root.findall('sentence'):\n",
    "            # Get sentence id and tokenize text\n",
    "            sent_id = sentence.get('id') # get sentence id\n",
    "            sent_text = sentence.get('text') #get sentence text \n",
    "            # load sentence entities into a dictionary\n",
    "            entities = {}\n",
    "            for ent in sentence.findall('entity'):\n",
    "                ent_id = ent.get('id') \n",
    "                offs = ent.get('charOffset').split('-')\n",
    "                entities[ent_id] = offs\n",
    "            # Tokenize, tag and parse sentence\n",
    "            analysis = analyze(sent_text)\n",
    "            # for each pair in the sentence, decide whether it is DDI and its type\n",
    "            for pair in sentence.findall('pair'):\n",
    "                id_e1 = pair.get('e1')\n",
    "                id_e2 = pair.get('e2')\n",
    "                (is_ddi, ddi_type) = check_interaction(analysis, entities, id_e1, id_e2)\n",
    "    # get performance score\n",
    "    evaluate(inputdir, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_function(inputdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
