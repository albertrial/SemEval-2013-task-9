{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab 3: DDI \n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.parse.corenlp import CoreNLPDependencyParser\n",
    "from nltk.corpus import stopwords \n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "devel_path = '../../data/Devel'\n",
    "test_path = '../../data/Test-DDI'\n",
    "train_path = '../../data/Train'\n",
    "outputfile = 'task9.2_develGoal_1.txt'\n",
    "\n",
    "my_parser = CoreNLPDependencyParser(url=\"http://localhost:9000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_symbols(node):\n",
    "    if node['word'] == '-LRB-':\n",
    "        node['word'] = '('\n",
    "        node['lemma'] = '('\n",
    "    elif node['word'] == '-RRB-':\n",
    "        node['word'] = ')'\n",
    "        node['lemma'] = ')'\n",
    "    elif node['word'] == '-LSB-':\n",
    "        node['word'] = '['\n",
    "        node['lemma'] = '['\n",
    "    elif node['word'] == '-RSB-':\n",
    "        node['word'] = ']'\n",
    "        node['lemma'] = ']'\n",
    "    elif node['word'] in [\"``\", \"''\"]:\n",
    "        node['word'] = '\"'\n",
    "        node['lemma'] = '\"'\n",
    "    return\n",
    "    \n",
    "def analyze(sent):\n",
    "    if len(sent)<= 0:\n",
    "        return None\n",
    "    \n",
    "    mytree, = my_parser.raw_parse(sent)\n",
    "    tree = mytree.nodes\n",
    "    ini_token = 0\n",
    "                   \n",
    "    # clean tree\n",
    "    info = ['address', 'head', 'lemma', 'rel', 'word', 'tag']\n",
    "    \n",
    "    for k in sorted(tree.keys()):\n",
    "        node = tree[k] \n",
    "        for key in list(node):\n",
    "            if key not in info:\n",
    "                del node[key]\n",
    "        \n",
    "        handle_special_symbols(node)\n",
    "        \n",
    "        if k != 0:\n",
    "            # add offsets\n",
    "            ini_token = sent.find(node['word'] ,ini_token)\n",
    "            node['start'] = ini_token\n",
    "            ini_token += len(node['word'])\n",
    "            node['end'] = ini_token - 1\n",
    "            \n",
    "    return tree\n",
    "\n",
    "def get_entity_nodes(tree, entities, e1, e2):\n",
    "    entity1 = []\n",
    "    entity2 = []\n",
    "\n",
    "    starts1 = [offs[0] for offs in entities[e1]]\n",
    "    starts2 = [offs[0] for offs in entities[e2]]\n",
    "    ends1 = [offs[1] for offs in entities[e1]]\n",
    "    ends2 = [offs[1] for offs in entities[e2]]\n",
    "    \n",
    "    for k in sorted(tree.keys()):\n",
    "        if 'start' in tree[k].keys():\n",
    "            for i in range(len(starts1)):\n",
    "                if int(starts1[i]) in range(tree[k]['start'], tree[k]['end']+1) or int(ends1[i]) in range(tree[k]['start'], tree[k]['end']+1):\n",
    "                    entity1.append(tree[k])\n",
    "                elif tree[k]['start'] in range(int(starts1[i]), int(ends1[i])+1) and tree[k]['end'] in range(int(starts1[i]), int(ends1[i])+1):\n",
    "                    entity1.append(tree[k])\n",
    "                    \n",
    "            for i in range(len(starts2)):\n",
    "                if int(starts2[i]) in range(tree[k]['start'], tree[k]['end']+1) or int(ends2[i]) in range(tree[k]['start'], tree[k]['end']+1):\n",
    "                    entity2.append(tree[k])\n",
    "                elif tree[k]['start'] in range(int(starts2[i]), int(ends2[i])+1) and tree[k]['end'] in range(int(starts2[i]), int(ends2[i])+1):\n",
    "                    entity2.append(tree[k])\n",
    "                    \n",
    "    return entity1, entity2\n",
    "\n",
    "def check_interaction(analysis, entities, e1, e2):\n",
    "    # Get entities\n",
    "    entity1, entity2 = get_entity_nodes(analysis, entities, e1, e2)\n",
    "    \n",
    "    # DDI rules\n",
    "    # e1_e2_under_same_verb -> \"advise\"\n",
    "    # e1_e2_under_same_word_but_not_noun_or_verb -> none\n",
    "    # e1_under_e2 -> none\n",
    "    for ent1 in entity1:\n",
    "        if analysis[ent1['head']] in entity2:\n",
    "            return (0, \"null\")\n",
    "        \n",
    "    for ent1 in entity1:\n",
    "        for ent2 in entity2:\n",
    "            if ent1['head'] == ent2['head'] and analysis[ent1['head']]['tag'].lower()[0] not in ['v', 'n']:\n",
    "                return (0, \"null\")\n",
    "                \n",
    "            if ent1['head'] == ent2['head'] and analysis[ent1['head']]['tag'].lower()[0] == 'v':\n",
    "                return (1, \"advise\")\n",
    "    \n",
    "        # under:\n",
    "        # effect\n",
    "        # e1 under -> [response, diminish]\n",
    "        # e2 under -> [effect]\n",
    "        \n",
    "        # mechanism\n",
    "        # e1 or e2 under -> [concentration, absorption]\n",
    "        \n",
    "        # int\n",
    "        # e1 under -> [interact]\n",
    "        \n",
    "        # advise:\n",
    "        # e1 or e2 under -> [take]\n",
    "    for e in entity1:\n",
    "        if analysis[e['head']]['lemma'] in ['response', 'diminish', 'enhance']:\n",
    "            return (1, \"effect\")\n",
    "        elif analysis[e['head']]['lemma'] in ['absorption', 'metabolism', 'presence']:\n",
    "            return (1, \"mechanism\")\n",
    "        elif analysis[e['head']]['lemma'] in ['interact', 'interaction']:\n",
    "            return (1, \"int\")\n",
    "        elif analysis[e['head']]['lemma'] in ['take', 'adjustment', 'avoid', 'recommend', 'contraindicate']:\n",
    "            return (1, \"advise\")\n",
    "            \n",
    "    for e in entity2:\n",
    "        if analysis[e['head']]['lemma'] in ['effect']:\n",
    "            return (1, \"effect\")\n",
    "        elif analysis[e['head']]['lemma'] in ['absorption', 'metabolism', 'level', 'clearance']:\n",
    "            return (1, \"mechanism\")\n",
    "        elif analysis[e['head']]['lemma'] in ['take', 'caution']:\n",
    "            return (1, \"advise\")\n",
    "         \n",
    "        # Clue words\n",
    "#         type_effect = ['potentiate', 'prevent', 'elevation', 'response', 'effects', 'effect']\n",
    "#         type_mechanism = ['reduce', 'increase', 'decrease', 'inhibit', 'concentrations']\n",
    "#         type_int = ['interaction', 'interact' ,'following']\n",
    "#         type_advise = ['should', 'caution']\n",
    "\n",
    "#         type_effect = ['potentiate', 'prevent', 'elevation']\n",
    "#         type_mechanism = ['concentrations']\n",
    "#         type_int = ['interaction', 'interact']\n",
    "#         type_advise = ['should', 'caution']\n",
    "        \n",
    "#         between_words = []\n",
    "#         for key in analysis.keys():\n",
    "#             end_entity1 = max([entity1[i]['end'] for i in range(len(entity1))])\n",
    "#             start_entity2 = min([entity2[i]['start'] for i in range(len(entity2))])\n",
    "#             if 'start' in analysis[key].keys() and end_entity1 < analysis[key]['start'] < start_entity2:\n",
    "#                 between_words.append(analysis[key]['lemma'])\n",
    "    \n",
    "#         if len(list(set(type_effect) & set(between_words)))> 0:\n",
    "#             return (1,\"effect\")\n",
    "#         elif len(list(set(type_mechanism) & set(between_words)))> 0:\n",
    "#             return (1,\"mechanism\")\n",
    "#         elif len(list(set(type_int) & set(between_words)))> 0:\n",
    "#             return (1,\"int\")\n",
    "#         elif len(list(set(type_advise) & set(between_words)))> 0:\n",
    "#             return (1,\"advise\")\n",
    "        \n",
    "    return (0, \"null\")    \n",
    "    \n",
    "\n",
    "def evaluate(inputdir, outputfile):\n",
    "    os.system(\"java -jar ../../eval/evaluateDDI.jar \"+ str(inputdir) + \" \" + str(outputfile))\n",
    "    return \n",
    "\n",
    "def main_function(inputdir):\n",
    "    outf = open(outputfile, \"w\")\n",
    "    # process each file in directory\n",
    "    for filename in os.listdir(inputdir):\n",
    "        #print(filename)\n",
    "        # parse XML file, obtaining a DOM tree\n",
    "        fullname = os.path.join(inputdir, filename)\n",
    "        tree = ET.parse(fullname)\n",
    "        root = tree.getroot()  \n",
    "        \n",
    "        # process each sentence in the file\n",
    "        for sentence in root.findall('sentence'):\n",
    "            # Get sentence id and tokenize text\n",
    "            sent_id = sentence.get('id') # get sentence id\n",
    "            sent_text = sentence.get('text') #get sentence text\n",
    "            splitted_sent = sent_text.split('\\r\\n')\n",
    "            sent_text = splitted_sent[0]\n",
    "            for s in splitted_sent[1:]:\n",
    "                if len(s) >= 1:\n",
    "                    sent_text += ('. ' + s)\n",
    "            \n",
    "            # load sentence entities into a dictionary\n",
    "            entities = {}\n",
    "            for ent in sentence.findall('entity'):\n",
    "                ent_id = ent.get('id')\n",
    "                offsets = ent.get('charOffset').split(';')\n",
    "                offs = [o.split('-') for o in offsets]\n",
    "                entities[ent_id] = offs\n",
    "                \n",
    "            # Tokenize, tag and parse sentence\n",
    "            analysis = analyze(sent_text)\n",
    "            # for each pair in the sentence, decide whether it is DDI and its type\n",
    "            for pair in sentence.findall('pair'):\n",
    "                id_e1 = pair.get('e1')\n",
    "                id_e2 = pair.get('e2')\n",
    "\n",
    "                (is_ddi, ddi_type) = check_interaction(analysis, entities, id_e1, id_e2)\n",
    "      \n",
    "                outf.write(str(sent_id)+'|'+str(id_e1)+'|'+str(id_e2)+'|'+str(is_ddi)+'|'+str(ddi_type))\n",
    "                outf.write(\"\\n\")\n",
    "    outf.close()\n",
    "    # get performance score\n",
    "    evaluate(inputdir, outputfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "main_function(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
