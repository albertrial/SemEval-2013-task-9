{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - Second delivery - Task 9.2 DDI\n",
    "**Albert Rial**   \n",
    "**Karen Lliguin**   \n",
    "\n",
    "This delivery consists of solving the task 9.2 of the SemEval-2013 challenge. The task concerns classifying drug-drug interactions between pairs of drugs. \n",
    "\n",
    "The dataset provided contains XML files with sentences with drugs entities appearing on it and the corresponding interaction. There are four general types: mechanism, int, effect and advise. The data is already splitted in three subsets: Train, Devel and Test.\n",
    "\n",
    "To complete this task different methods and resources have been used among them, the Stanford CoreNLP dependency parser. We have divide the task in different subgoals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 1: Rule-based DDI\n",
    "### Introduction\n",
    "\n",
    "First, simple heur√≠stic rules have been used to carry out the task. In this version only the information given by the Train dataset is used. The final goal is to achieve an overall F1 score of at least 0.15 on the Devel dataset.\n",
    "\n",
    "### Data exploration\n",
    "\n",
    "\n",
    "With the purpose of building significant rules (and features for the next goal), first we have done a data exploration over the Train dataset. The following aspects have been analysed for each type of interaction:\n",
    "- The most common words that appear in between the two drugs that interact (clue_words).\n",
    "- The most common words that appear in each sentence containing drugs that interact (sentence_words).\n",
    "- The most common lemmas in which entity1 is under in the dependency tree (e1_under), its relation and the POS tag.\n",
    "- The most common lemmas in which entity2 is under in the dependency tree (e2_under), its relation and the POS tag.\n",
    "- Number of times entity1 is under entity2 and vice versa.\n",
    "- Number of times entity1 and entity2 share the same parent and how many times is a verb.\n",
    "\n",
    "Besides this analysis, in order to have a more clear understading of each metric regarding on how one word is seen by all the different types, we have stored the previous information so that we could search a word regarding any of the previous metrics and the information of how many times it appears for each type is shown. For intance, given the word \"response\" and the metric \"e1_under\" the following information is obtained:\n",
    "```\n",
    "--------------------\n",
    "effect\n",
    "meet_condition: 81\n",
    "total: 1525\n",
    "--------------------\n",
    "mechanism\n",
    "meet_condition: 0\n",
    "total: 1118\n",
    "--------------------\n",
    "int\n",
    "meet_condition: 0\n",
    "total: 186\n",
    "--------------------\n",
    "advise\n",
    "meet_condition: 0\n",
    "total: 707\n",
    "--------------------\n",
    "none\n",
    "meet_condition: 14\n",
    "total: 21553\n",
    "```\n",
    "This inside information is used to build the rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "\n",
    "The *analyse* function recives a sentence text and using CoreNLP it obtains the dependency graph, cleans it keeping only the relevant information and it adds the start/end to each token. Since Standford CoreNLP changes some characters (for instance parenthesis) to a string value. To deal with this we have implemented the function *handle_special_symbols* that before adding the offset it changes it back to the real character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_symbols(node):\n",
    "    if node['word'] == '-LRB-':\n",
    "        node['word'] = '('\n",
    "        node['lemma'] = '('\n",
    "    elif node['word'] == '-RRB-':\n",
    "        node['word'] = ')'\n",
    "        node['lemma'] = ')'\n",
    "    elif node['word'] == '-LSB-':\n",
    "        node['word'] = '['\n",
    "        node['lemma'] = '['\n",
    "    elif node['word'] == '-RSB-':\n",
    "        node['word'] = ']'\n",
    "        node['lemma'] = ']'\n",
    "    elif node['word'] in [\"``\", \"''\"]:\n",
    "        node['word'] = '\"'\n",
    "        node['lemma'] = '\"'\n",
    "    return\n",
    "    \n",
    "def analyze(sent):\n",
    "    if len(sent)<= 0:\n",
    "        return None\n",
    "    \n",
    "    mytree, = my_parser.raw_parse(sent)\n",
    "    tree = mytree.nodes\n",
    "    ini_token = 0\n",
    "                   \n",
    "    # clean tree\n",
    "    info = ['address', 'head', 'lemma', 'rel', 'word', 'tag']\n",
    "    \n",
    "    for k in sorted(tree.keys()):\n",
    "        node = tree[k] \n",
    "        for key in list(node):\n",
    "            if key not in info:\n",
    "                del node[key]\n",
    "        \n",
    "        handle_special_symbols(node)\n",
    "        \n",
    "        if k != 0:\n",
    "            # add offsets\n",
    "            ini_token = sent.find(node['word'] ,ini_token)\n",
    "            node['start'] = ini_token\n",
    "            ini_token += len(node['word'])\n",
    "            node['end'] = ini_token - 1\n",
    "            \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules\n",
    "\n",
    "**Discard**\n",
    "\n",
    "As a first approach, we decided to use the \"clue_words\" found in data exploration but since this approach is very naive, the results obtained were not good enough so we discarded this option.\n",
    "\n",
    "**Used**\n",
    "Therefore we change the strategy and decided to use the relation \"entityA under entityB\". With this approach and using the information of the data exploration we explored rules that check the lemma in which entity1/entity2 is under in the dependency tree. After trying different lemmas for each type of interacction, the final lemmas selected for each type interaction are the following:\n",
    "\n",
    "- Effect: response,diminish and enhance\n",
    "- Int: interact and interaction\n",
    "- Mechanism: absorption, metabolism and presence\n",
    "- Advise: take, adjustment, avoid, recommend and contraindicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interaction(analysis, entities, e1, e2):\n",
    "    # Get entities\n",
    "    entity1, entity2 = get_entity_nodes(analysis, entities, e1, e2)\n",
    "    parent1, rel1 = get_entity_parent(analysis, entity1)\n",
    "    parent2, rel2 = get_entity_parent(analysis, entity2)\n",
    "    \n",
    "    # Rules\n",
    "    # Entity 1 is under Entity 2\n",
    "    if is_under(entity1, entity2):\n",
    "        return (0, \"null\")\n",
    "    \n",
    "    # Entities under same parent\n",
    "    if same(parent1, parent2):\n",
    "        tag = parent1['tag'].lower()[0]\n",
    "        if tag not in ['v', 'n']:\n",
    "            return (0, \"null\")\n",
    "        if tag == 'v':\n",
    "            return (1, \"advise\")\n",
    "    \n",
    "    # Entity 1 under lemma\n",
    "    if parent_lemma_belongs(parent1, ['response', 'diminish', 'enhance']):\n",
    "        return (1, \"effect\")\n",
    "    elif parent_lemma_belongs(parent1, ['absorption', 'metabolism', 'presence']):\n",
    "        return (1, \"mechanism\")\n",
    "    elif parent_lemma_belongs(parent1, ['interact', 'interaction']):\n",
    "        return (1, \"int\")\n",
    "    elif parent_lemma_belongs(parent1, ['take', 'adjustment', 'avoid', 'recommend', 'contraindicate']):\n",
    "        return (1, \"advise\")\n",
    "\n",
    "    # Entity 2 under lemma\n",
    "    if parent_lemma_belongs(parent2, ['effect']):\n",
    "        return (1, \"effect\")\n",
    "    elif parent_lemma_belongs(parent2, ['absorption', 'metabolism', 'level', 'clearance']):\n",
    "        return (1, \"mechanism\")\n",
    "    elif parent_lemma_belongs(parent2, ['take', 'caution']):\n",
    "        return (1, \"advise\")\n",
    "        \n",
    "    return (0, \"null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Brief description of auxiliary functions\n",
    "\n",
    "The *check_interaction* function uses some external functions:\n",
    "- **get_entity_nodes:** returns the corresponding node and tokens of each entity.\n",
    "- **get_entity_parent:** for each entity it returns the parent anf if it has several tokens it returns an arbitrary parent.\n",
    "- **is_under:** given two entities it returns True if one entity is under the other.\n",
    "- **same:** given two nodes it returns True if the both have the addres. \n",
    "- **parent_lemma_belongs:** given a lemma and a lemma set it returns true it the lemma belong to the set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_nodes(tree, entities, e1, e2):\n",
    "    entity1 = []\n",
    "    entity2 = []\n",
    "\n",
    "    starts1 = [offs[0] for offs in entities[e1]]\n",
    "    starts2 = [offs[0] for offs in entities[e2]]\n",
    "    ends1 = [offs[1] for offs in entities[e1]]\n",
    "    ends2 = [offs[1] for offs in entities[e2]]\n",
    "    \n",
    "    for k in sorted(tree.keys()):\n",
    "        if 'start' in tree[k].keys():\n",
    "            for i in range(len(starts1)):\n",
    "                if int(starts1[i]) in range(tree[k]['start'], tree[k]['end']+1) or int(ends1[i]) in range(tree[k]['start'], tree[k]['end']+1):\n",
    "                    entity1.append(tree[k])\n",
    "                elif tree[k]['start'] in range(int(starts1[i]), int(ends1[i])+1) and tree[k]['end'] in range(int(starts1[i]), int(ends1[i])+1):\n",
    "                    entity1.append(tree[k])\n",
    "                    \n",
    "            for i in range(len(starts2)):\n",
    "                if int(starts2[i]) in range(tree[k]['start'], tree[k]['end']+1) or int(ends2[i]) in range(tree[k]['start'], tree[k]['end']+1):\n",
    "                    entity2.append(tree[k])\n",
    "                elif tree[k]['start'] in range(int(starts2[i]), int(ends2[i])+1) and tree[k]['end'] in range(int(starts2[i]), int(ends2[i])+1):\n",
    "                    entity2.append(tree[k])\n",
    "                    \n",
    "    return entity1, entity2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_parent(tree, entity):\n",
    "    if len(entity) == 1:\n",
    "        return tree[entity[0]['head']], entity[0]['rel']\n",
    "    else:\n",
    "        parent = None\n",
    "        rel = None\n",
    "        for e in entity:\n",
    "            if e['head'] not in [other['address'] for other in entity]:\n",
    "                parent = tree[e['head']]\n",
    "                rel = e['rel']\n",
    "        return parent, rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_under(entity1, entity2):\n",
    "    for i in range(len(entity1)):\n",
    "        if entity1[i]['head'] in [e['address'] for e in entity2]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same(node1, node2):\n",
    "    if node1['address'] == node2['address']:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_lemma_belongs(parent, lemma_set):\n",
    "    if parent['lemma'] in lemma_set:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "The final results obtained for devel y test are shown below. \n",
    "\n",
    "#### Devel\n",
    "```\n",
    "SCORES FOR THE GROUP: develGoal RUN=1\n",
    "Gold Dataset: /Devel\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "136     250     348     484     0,3523  0,281   0,3126\n",
    "\n",
    "\n",
    "Detection and classification of DDI\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "123     263     361     484     0,3187  0,2541  0,2828\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "60      135     141     201     0,3077  0,2985  0,303\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "44      57      118     162     0,4356  0,2716  0,3346\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "17      38      102     119     0,3091  0,1429  0,1954\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "2       33      0       2       0,0571  1       0,1081\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES FOR DDI CLASSIFICATION:\n",
    "        P       R       F1\n",
    "        0,2774  0,4282  0,3367\n",
    "________________________________________________________________________\n",
    "```\n",
    "#### Test\n",
    "```\n",
    "SCORES FOR THE GROUP: testGoal RUN=1\n",
    "Gold Dataset: /Test-DDI\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "262     495     717     979     0,3461  0,2676  0,3018\n",
    "\n",
    "\n",
    "Detection and classification of DDI\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "215     542     764     979     0,284   0,2196  0,2477\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "88      192     214     302     0,3143  0,2914  0,3024\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "49      54      311     360     0,4757  0,1361  0,2117\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "32      137     189     221     0,1893  0,1448  0,1641\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "46      159     50      96      0,2244  0,4792  0,3056\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES FOR DDI CLASSIFICATION:\n",
    "        P       R       F1\n",
    "        0,3009  0,2629  0,2806\n",
    "________________________________________________________________________\n",
    "```\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "The final results obtained are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2: DDI using machine learning\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rule-based system implemented clearly has a lot of limitations. It is unable to reach a quite good accuracy in both Devel and Test datasets. For this reason, in this task we will use a machine learning approach.\n",
    "\n",
    "#### ML algorithm\n",
    "Specifically we use a Maximum Entropy model.\n",
    "\n",
    "**TO-DO**\n",
    "JUSTIFICATION\n",
    "\n",
    "This time the goal is to achieve an overall F1 score of at least 0.6 on Devel dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features\n",
    "#### Used features\n",
    "\n",
    "#### Discarded features\n",
    "- Using the Dependency Tree, we also tested features with information about the distance of each entitiy to the common head between both entities (distance between the common parent and entity1, distance between common parent and entity2, and the sum of both).\n",
    "- Words and POS tags of tokens before, in between and after the entities (in the same way to what we finally do with lemmas).\n",
    "- Feature indicating if the parent lemma of one entity belongs to a set of lemmas, one for each type of drug-drug interaction. In fact, these sets of lemmas used were the ones extracted from the data exploration and used also in the rule-based system. After testing, we end up with only maintaining it for the interaction type.\n",
    "- We also tested a feature indicating if both entities were under same head and this was a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tree, entities, e1, e2):\n",
    "    # Get entities\n",
    "    entity1, entity2 = get_entity_nodes(tree, entities, e1, e2)\n",
    "    \n",
    "    # Get head and rel of each entity\n",
    "    parent1, rel1 = get_entity_parent(tree, entity1)\n",
    "    parent2, rel2 = get_entity_parent(tree, entity2)\n",
    "    \n",
    "    # Features\n",
    "    features = ['h1_lemma=%s' % parent1['lemma'],\n",
    "                'h1_word=%s' % parent1['word'],\n",
    "                'h1_tag=%s' % parent1['tag'],\n",
    "                'h1_rel=%s' % rel1,\n",
    "                'h2_lemma=%s' % parent2['lemma'],\n",
    "                'h2_word=%s' % parent2['word'],\n",
    "                'h2_tag=%s' % parent2['tag'],\n",
    "                'h2_rel=%s' % rel2,\n",
    "                ]\n",
    "    \n",
    "    if same(parent1, parent2):\n",
    "        features.append('under_same')\n",
    "        if parent1['tag'][0].lower() == 'v':\n",
    "            features.append('under_same_verb')\n",
    "    \n",
    "    if is_under(entity1, entity2):\n",
    "        features.append('1under2')\n",
    "    \n",
    "    if is_under(entity2, entity1):\n",
    "        features.append('2under1')\n",
    "        \n",
    "    if parent_lemma_belongs(parent1, ['interact', 'interaction']):\n",
    "        features.append('interaction')\n",
    "    \n",
    "    start1, end1 = get_entity_limits(entity1)\n",
    "    start2, end2 = get_entity_limits(entity2)\n",
    "    for k in sorted(tree.keys()):\n",
    "        if 'start' in tree[k].keys() and  tree[k]['start'] < start1:\n",
    "            features.append('lb1=%s' % tree[k]['lemma'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end1 < tree[k]['start'] < start2:\n",
    "            features.append('lib=%s' % tree[k]['lemma'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end2 < tree[k]['start']:\n",
    "            features.append('la2=%s' % tree[k]['lemma'])\n",
    "      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_limits(entity):\n",
    "    start = min([e['start'] for e in entity])\n",
    "    end = max([e['end'] for e in entity])\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(megam, parameters, features_file, out_train_model):\n",
    "    print(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" multiclass \" + features_file + \" > \" + out_train_model + \"\\\"\")\n",
    "    os.system(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" multiclass \" + features_file + \" > \" + out_train_model + \"\\\"\")### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = '-quiet -nc -repeat 5 -tune -lambda 0.01 -minfc 3'\n",
    "train(model_path+'megam.opt', parameters, features_path+'train_features', model_path+'model.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(megam, parameters, features_file, train_model, prediction_output):\n",
    "    print(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" -predict \" + train_model + \" multiclass \" + features_file + \" > \" + prediction_output + \"\\\"\")\n",
    "    os.system(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" -predict \" + train_model + \" multiclass \" + features_file + \" > \" + prediction_output + \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devel prediction\n",
    "classify(model_path+'megam.opt', '-quiet -nc ', features_path+'devel_features', model_path+'model.dat', output_path+'devel_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "classify(model_path+'megam.opt', '-quiet -nc ', features_path+'test_features', model_path+'model.dat', output_path+'test_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "#### Devel\n",
    "```\n",
    "SCORES FOR the file: task9.2_develGoal.txt\n",
    "Gold Dataset: /Devel\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "260\t99\t224\t484\t0,7242\t0,5372\t0,6168\n",
    "\n",
    "\n",
    "Detection and Classification of DDI\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "240\t119\t244\t484\t0,6685\t0,4959\t0,5694\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "86\t65\t115\t201\t0,5695\t0,4279\t0,4886\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "99\t37\t63\t162\t0,7279\t0,6111\t0,6644\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "53\t16\t66\t119\t0,7681\t0,4454\t0,5638\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "2\t1\t0\t2\t0,6667\t1\t0,8\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES:\n",
    "\tP\tR\tF1\n",
    "\t0,6831\t0,6211\t0,6506\n",
    "________________________________________________________________________\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "SCORES FOR the file: task9.2_testGoal.txt\n",
    "Gold Dataset: /Test-DDI\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "714\t521\t265\t979\t0,5781\t0,7293\t0,645\n",
    "\n",
    "\n",
    "Detection and Classification of DDI\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "619\t616\t360\t979\t0,5012\t0,6323\t0,5592\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "214\t213\t88\t302\t0,5012\t0,7086\t0,5871\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "222\t221\t138\t360\t0,5011\t0,6167\t0,5529\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "146\t156\t75\t221\t0,4834\t0,6606\t0,5583\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp\tfp\tfn\ttotal\tprec\trecall\tF1\n",
    "37\t26\t59\t96\t0,5873\t0,3854\t0,4654\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES:\n",
    "\tP\tR\tF1\n",
    "\t0,5183\t0,5928\t0,553\n",
    "________________________________________________________________________\n",
    "```\n",
    "\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "The final results obtained are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
