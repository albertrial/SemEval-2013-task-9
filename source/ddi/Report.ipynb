{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - Second delivery - Task 9.2 DDI\n",
    "**Albert Rial**   \n",
    "**Karen Lliguin**   \n",
    "\n",
    "This delivery consists of solving the task 9.2 of the SemEval-2013 challenge. The task concerns the detection and classification of drug-drug interactions between pairs of drugs.\n",
    "\n",
    "The dataset provided contains XML files with sentences and the drugs appearing on them as well as their interactions. An interaction can be of four general types: \n",
    "- Mechanism: type assigned when it is described the proccess by which drugs are absorbed, distributed, metabolised and excreted.\n",
    "- Effect: this type is assigned when the effect of the drug-drug interaction is described.\n",
    "- Advise: this category is assigned to those drug-drug interactions where a recommendation or advise is described.\n",
    "- Interaction: this type is assigned when a sentence simply states that an interaction occurs but does not provide any more information.\n",
    "\n",
    "The data is already splitted in three subsets: Train, Devel and Test. The Train dataset will be used to extract information and train our models. The Devel will be used for validation and test purposes (to see where we need to improve, to optimize the performance and ensure generalization is preserved). Finally, the Test set will only be used to test our final model. \n",
    "\n",
    "The evaluation is done in terms of the F1 score, which combines the results of precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 1: Rule-based DDI\n",
    "### Introduction\n",
    "\n",
    "First, simple heur√≠stic rules have been used to carry out the task. In this version only the information given by the Train dataset is used. The final goal is to achieve an overall F1 score of at least 0.15 on the Devel dataset.\n",
    "\n",
    "### Data exploration\n",
    "\n",
    "\n",
    "With the purpose of building significant rules (and features for the next goal), first we have done a data exploration over the Train dataset. The following aspects have been analysed for each type of interaction:\n",
    "- The most common words that appear in between the two drugs that interact (clue_words).\n",
    "- The most common words that appear in each sentence containing drugs that interact (sentence_words).\n",
    "- The most common lemmas in which entity1 is under in the dependency tree (e1_under), its relation and the POS tag.\n",
    "- The most common lemmas in which entity2 is under in the dependency tree (e2_under), its relation and the POS tag.\n",
    "- Number of times entity1 is under entity2 and vice versa.\n",
    "- Number of times entity1 and entity2 share the same parent and how many times is a verb.\n",
    "\n",
    "Besides this analysis, in order to have a more clear understading of each metric regarding on how one word is seen by all the different types, we have stored the previous information so that we could search a word regarding any of the previous metrics and the information of how many times it appears for each type is shown. For intance, given the word \"response\" and the metric \"e1_under\" the following information is obtained:\n",
    "```\n",
    "--------------------\n",
    "effect\n",
    "meet_condition: 81\n",
    "total: 1525\n",
    "--------------------\n",
    "mechanism\n",
    "meet_condition: 0\n",
    "total: 1118\n",
    "--------------------\n",
    "int\n",
    "meet_condition: 0\n",
    "total: 186\n",
    "--------------------\n",
    "advise\n",
    "meet_condition: 0\n",
    "total: 707\n",
    "--------------------\n",
    "none\n",
    "meet_condition: 14\n",
    "total: 21553\n",
    "```\n",
    "This inside information is used to build the rules and also features of the Goal 2 we will se later.\n",
    "\n",
    "### Details\n",
    "\n",
    "The **analyse** function recives a sentence text and using CoreNLP it obtains the dependency graph, cleans it keeping only the relevant information and it adds the start/end of each token. Since we found that Standford CoreNLP changes some special characters (parenthesis, double comma, ...) for special symbols like \"-LRB-\" or \"-RSB-\", we have implemented the function **handle_special_symbols** that changes these special symbols back to the real character. We do this because we need this real character in order to assign correctly the start/end of each token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_symbols(node):\n",
    "    if node['word'] == '-LRB-':\n",
    "        node['word'] = '('\n",
    "        node['lemma'] = '('\n",
    "    elif node['word'] == '-RRB-':\n",
    "        node['word'] = ')'\n",
    "        node['lemma'] = ')'\n",
    "    elif node['word'] == '-LSB-':\n",
    "        node['word'] = '['\n",
    "        node['lemma'] = '['\n",
    "    elif node['word'] == '-RSB-':\n",
    "        node['word'] = ']'\n",
    "        node['lemma'] = ']'\n",
    "    elif node['word'] in [\"``\", \"''\"]:\n",
    "        node['word'] = '\"'\n",
    "        node['lemma'] = '\"'\n",
    "    return\n",
    "    \n",
    "def analyze(sent):\n",
    "    if len(sent)<= 0:\n",
    "        return None\n",
    "    \n",
    "    mytree, = my_parser.raw_parse(sent)\n",
    "    tree = mytree.nodes\n",
    "    ini_token = 0\n",
    "                   \n",
    "    # clean tree\n",
    "    info = ['address', 'head', 'lemma', 'rel', 'word', 'tag']\n",
    "    \n",
    "    for k in sorted(tree.keys()):\n",
    "        node = tree[k] \n",
    "        for key in list(node):\n",
    "            if key not in info:\n",
    "                del node[key]\n",
    "        \n",
    "        handle_special_symbols(node)\n",
    "        \n",
    "        if k != 0:\n",
    "            # add offsets\n",
    "            ini_token = sent.find(node['word'] ,ini_token)\n",
    "            node['start'] = ini_token\n",
    "            ini_token += len(node['word'])\n",
    "            node['end'] = ini_token - 1\n",
    "            \n",
    "    return tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules\n",
    "\n",
    "##### Discarded rules\n",
    "As a first approach, we decided to use the clue words found in data exploration (lemmas/words between both entities) but since this approach is very naive, the results obtained were not good enough so we discarded this option.\n",
    "\n",
    "##### Used rules\n",
    "Therefore, we change the strategy and decided to use the relation \"entity under parent\". With this approach and using the information of the data exploration we explored rules that check the lemma in which entity1/entity2 is under in the dependency tree. If the entity is under a certain lemma we assign it to the type of interaction we have associated with that lemma. After trying different lemmas for each type of interacction, the final lemmas selected for each type interaction are the following:\n",
    "\n",
    "- Effect: response, diminish and enhance for entity1 / effect for entity2\n",
    "- Int: interact and interaction for entity1\n",
    "- Mechanism: absorption, metabolism and presence for entity1 / absorption, metabolism, level and clearance for entity2\n",
    "- Advise: take, adjustment, avoid, recommend and contraindicate for entity1 / take and caution for entity2\n",
    "\n",
    "To decrease the false positives of each type interaction we introduced some previous rules to discard those false positives and finally reach a must better result:\n",
    "- If the entity1 is under entity2, we assume there is no interaction so we return the null type. We do this because we saw that in Train set this condition between entities was only happening when there was no interaction.\n",
    "- If the entities have the same parent but is not a verb or noun, we also say there is no interaction.\n",
    "- Finally if the entities have the same parent but this is a verb, we assign the advise type, since in data exploration we saw this condition was commonly occurring in this type of interaction.\n",
    "\n",
    "All these rules are implemented inside the **check_interaction** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_interaction(analysis, entities, e1, e2):\n",
    "    # Get entities\n",
    "    entity1, entity2 = get_entity_nodes(analysis, entities, e1, e2)\n",
    "    parent1, rel1 = get_entity_parent(analysis, entity1)\n",
    "    parent2, rel2 = get_entity_parent(analysis, entity2)\n",
    "    \n",
    "    # Rules\n",
    "    # Entity 1 is under Entity 2\n",
    "    if is_under(entity1, entity2):\n",
    "        return (0, \"null\")\n",
    "    \n",
    "    # Entities under same parent\n",
    "    if same(parent1, parent2):\n",
    "        tag = parent1['tag'].lower()[0]\n",
    "        if tag not in ['v', 'n']:\n",
    "            return (0, \"null\")\n",
    "        if tag == 'v':\n",
    "            return (1, \"advise\")\n",
    "    \n",
    "    # Entity 1 under lemma\n",
    "    if parent_lemma_belongs(parent1, ['response', 'diminish', 'enhance']):\n",
    "        return (1, \"effect\")\n",
    "    elif parent_lemma_belongs(parent1, ['absorption', 'metabolism', 'presence']):\n",
    "        return (1, \"mechanism\")\n",
    "    elif parent_lemma_belongs(parent1, ['interact', 'interaction']):\n",
    "        return (1, \"int\")\n",
    "    elif parent_lemma_belongs(parent1, ['take', 'adjustment', 'avoid', 'recommend', 'contraindicate']):\n",
    "        return (1, \"advise\")\n",
    "\n",
    "    # Entity 2 under lemma\n",
    "    if parent_lemma_belongs(parent2, ['effect']):\n",
    "        return (1, \"effect\")\n",
    "    elif parent_lemma_belongs(parent2, ['absorption', 'metabolism', 'level', 'clearance']):\n",
    "        return (1, \"mechanism\")\n",
    "    elif parent_lemma_belongs(parent2, ['take', 'caution']):\n",
    "        return (1, \"advise\")\n",
    "        \n",
    "    return (0, \"null\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above **check_interaction** function uses some auxiliary functions that handle the relations between entities, their parents, etc. This are very important since are able to handle entities with different tokens. In a first approach we were only able to handle entities with one token or with contiguous tokens, but not the rest. Then, when we changed this behavior and we started detecting and handling all entities we saw a huge improvement.\n",
    "\n",
    "**get_entity_nodes:** returns all the corresponding nodes of each entity. To do so, it iterates over all the tree and detects the entities based on the start and end found in analyze function and the offset obtained from the dataset. We are able to detect not contiguous entities and even those that are not tokenized correctly for CoreNLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_nodes(tree, entities, e1, e2):\n",
    "    entity1 = []\n",
    "    entity2 = []\n",
    "\n",
    "    starts1 = [offs[0] for offs in entities[e1]]\n",
    "    starts2 = [offs[0] for offs in entities[e2]]\n",
    "    ends1 = [offs[1] for offs in entities[e1]]\n",
    "    ends2 = [offs[1] for offs in entities[e2]]\n",
    "    \n",
    "    for k in sorted(tree.keys()):\n",
    "        if 'start' in tree[k].keys():\n",
    "            for i in range(len(starts1)):\n",
    "                if int(starts1[i]) in range(tree[k]['start'], tree[k]['end']+1) or int(ends1[i]) in range(tree[k]['start'], tree[k]['end']+1):\n",
    "                    entity1.append(tree[k])\n",
    "                elif tree[k]['start'] in range(int(starts1[i]), int(ends1[i])+1) and tree[k]['end'] in range(int(starts1[i]), int(ends1[i])+1):\n",
    "                    entity1.append(tree[k])\n",
    "                    \n",
    "            for i in range(len(starts2)):\n",
    "                if int(starts2[i]) in range(tree[k]['start'], tree[k]['end']+1) or int(ends2[i]) in range(tree[k]['start'], tree[k]['end']+1):\n",
    "                    entity2.append(tree[k])\n",
    "                elif tree[k]['start'] in range(int(starts2[i]), int(ends2[i])+1) and tree[k]['end'] in range(int(starts2[i]), int(ends2[i])+1):\n",
    "                    entity2.append(tree[k])\n",
    "                    \n",
    "    return entity1, entity2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**get_entity_parent:** given an entity (list of nodes) it returns the parent. If the entity has only one token/node it returns the head of that token. If it has several, it returns the parent that is outside of the tokens entity (if it has more than one outside, an arbitrary one is chosen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_parent(tree, entity):\n",
    "    if len(entity) == 1:\n",
    "        return tree[entity[0]['head']], entity[0]['rel']\n",
    "    else:\n",
    "        parent = None\n",
    "        rel = None\n",
    "        for e in entity:\n",
    "            if e['head'] not in [other['address'] for other in entity]:\n",
    "                parent = tree[e['head']]\n",
    "                rel = e['rel']\n",
    "        return parent, rel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**is_under:** given two entities (entity1 and entity2) it returns True if the entity1 is under the entity2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_under(entity1, entity2):\n",
    "    for i in range(len(entity1)):\n",
    "        if entity1[i]['head'] in [e['address'] for e in entity2]:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**same:** given two nodes it returns True if the both have the same address (are the same). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same(node1, node2):\n",
    "    if node1['address'] == node2['address']:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**parent_lemma_belongs:** given a node of the dependency tree and a lemma set, it returns true if the node lemma belongs to the set. As it is used for detecting if a given parent lemma belongs to an specific set, we named it this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parent_lemma_belongs(parent, lemma_set):\n",
    "    if parent['lemma'] in lemma_set:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The final results obtained for devel and test are shown below:\n",
    "\n",
    "#### Devel\n",
    "```\n",
    "SCORES FOR THE GROUP: develGoal RUN=1\n",
    "Gold Dataset: /Devel\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "136     250     348     484     0,3523  0,281   0,3126\n",
    "\n",
    "\n",
    "Detection and classification of DDI\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "123     263     361     484     0,3187  0,2541  0,2828\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "60      135     141     201     0,3077  0,2985  0,303\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "44      57      118     162     0,4356  0,2716  0,3346\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "17      38      102     119     0,3091  0,1429  0,1954\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "2       33      0       2       0,0571  1       0,1081\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES FOR DDI CLASSIFICATION:\n",
    "        P       R       F1\n",
    "        0,2774  0,4282  0,3367\n",
    "________________________________________________________________________\n",
    "```\n",
    "#### Test\n",
    "```\n",
    "SCORES FOR THE GROUP: testGoal RUN=1\n",
    "Gold Dataset: /Test-DDI\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "262     495     717     979     0,3461  0,2676  0,3018\n",
    "\n",
    "\n",
    "Detection and classification of DDI\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "215     542     764     979     0,284   0,2196  0,2477\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "88      192     214     302     0,3143  0,2914  0,3024\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "49      54      311     360     0,4757  0,1361  0,2117\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "32      137     189     221     0,1893  0,1448  0,1641\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "46      159     50      96      0,2244  0,4792  0,3056\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES FOR DDI CLASSIFICATION:\n",
    "        P       R       F1\n",
    "        0,3009  0,2629  0,2806\n",
    "________________________________________________________________________\n",
    "```\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "The global results are:\n",
    "\n",
    "**Devel: precision 0.2774, recall 0.4282 and F1 0.3367**\n",
    "\n",
    "**Test: precision 0.3009, recall 0.2629 and F1 0.2806**\n",
    "\n",
    "From the result obtained it can be observed that this approach is not the best one to tackle the goal stated. \n",
    "\n",
    "It can be seen that for all types of interaction except *int* the number of false negatives is quite high. This means that with the rules implemented most of the interaction between drugs is missed. This is because we have tried to achieve the goal F1 score but maintaning a good balance between the precision and recall (in fact, we tried to only add precise rules in order to have higher precision than recall). The precision value is always higher than recall except for *int* type.\n",
    "\n",
    "Even though in *int* interaction type we have a higher recall, we have been able to have a general good balance between global precision and recall. We do not take much into account the recall in devel dataset as there are only two interactions of type *int* in all the whole set, and when you detect both (*int* recall = 1), the global recall rises a lot. \n",
    "\n",
    "When defining the rules we saw that it was better to use the information of the dependency tree to detect and classify interactions, than using naive heuristics such as the clue words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2: DDI using machine learning\n",
    "### Introduction\n",
    "The rule-based system implemented, clearly has a lot of limitations. It was unable to reach a quite good accuracy in both Devel and Test datasets. For this reason, in this task we used a machine learning approach.\n",
    "\n",
    "This time, the goal is to achieve an overall F1 score of at least 0.6 on Devel dataset.\n",
    "\n",
    "### Details\n",
    "#### ML algorithm\n",
    "The machine learning approach used is the Maximum Entropy model. It was the algorithm chosen in a first instance because of its simplicity and efficiency. Later, we doubted whether this approach would be capable of learning a problem of this complexity so we also tested a SVM. However, the results we obtained were not as good as the ones we had with ME, so we decided to continue testing and tuning our first approach.\n",
    "\n",
    "The Maximum Entropy model implementation used has been **megam** seen in class. Is an efficient implementation of the ME and contains different parameters to tune its behaviour and find the sweet spot for the given problem. \n",
    "\n",
    "Among all the parameters that it has, we have tested the following ones:\n",
    "```\n",
    "- maxi <int> :specify the maximum number of iterations (default: 100)\n",
    "- lambda <float>: specify the precision of the Gaussian prior for maxent; or the value for C for passive-aggressive algorithms (default: 1)\n",
    "- tune: tune lambda using repeated optimizations (starts with specified -lambda value and drops by half each time until optimal dev error rate is achieved)\n",
    "- norm1: l1 normalization on instances\n",
    "- norm2: l2 normalization on instances\n",
    "- minfc <int>: remove all features with frequency <= <int>\n",
    "- nobias: do not use the bias features\n",
    "- repeat <int>: repeat optimization <int> times\n",
    "```\n",
    "We also pre-defined some parameters we did not change:\n",
    "```\n",
    "- nc: use named classes (rather than numbered)\n",
    "- <model-type>: we used the model type multiclass, since our problem is of this type.\n",
    "```\n",
    "\n",
    "After having chosen the algorithm and defined some basic features, we implemented and set ready our Feature extractor, Learner and Classifier. Then we started testing different features (adding, removing, modifying, ...) and different hyperparameters of the Maximum Entropy model. The performance was evaluated on the Devel dataset but also on the Train, to control the overfitting between them. Finally, we tested our best model and best subset of features on the Test dataset.\n",
    "\n",
    "\n",
    "#### Hyperparameters\n",
    "The best hyperparameters found for the ME model have been the following ones:\n",
    "```\n",
    "- maxi 100 (default)\n",
    "- lambda 0.01\n",
    "- tune\n",
    "- minfc 3\n",
    "- repeat 5\n",
    "```\n",
    "#### Features\n",
    "##### Used features\n",
    "The best set of features found after testing different subsets and their order (because we realized that order mattered) are the following ones (their implementation can be seen below inside the **extract_features** function):\n",
    "- Lemma, word and POS tag of the parent of each entity, and the relation between the entity and its parent.\n",
    "- Feature indicating if both entities are under the same parent.\n",
    "- Feature indicating if both entities are under the same parent and this is a verb.\n",
    "- Feature indicating if entity1 is under entity2 and vice versa.\n",
    "- Feature indicating if the parent lemma of entity1 is 'interact' or 'interaction' (specially useful to detect interaction types, as we saw in our rule-based system).\n",
    "- Lemmas before both entities, lemmas in between the entities and lemmas after both.\n",
    "\n",
    "As we can see, the features finally used are not very complex and most of them are similar to the rules used in our rule-based system. Nevertheless, they give the model a lot of useful information using the Dependency Tree and the tokens of the sentence.\n",
    "\n",
    "##### Discarded features\n",
    "On the other hand, the discarded features have been:\n",
    "- Using the Dependency Tree, we also tested features with information about the distance of each entitiy to the common head between both entities (distance between the common parent and entity1, distance between common parent and entity2, and the sum of both).\n",
    "- Words and POS tags of tokens before, in between and after the entities (in the same way to what we finally do with lemmas).\n",
    "- Feature indicating if the parent lemma of one entity belongs to a set of lemmas, one for each type of drug-drug interaction. In fact, these sets of lemmas used were the ones extracted from the data exploration and used also in the rule-based system. After testing, we end up with only maintaining it for the interaction type.\n",
    "- We also tested a feature indicating if both entities were under same head and this was a noun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tree, entities, e1, e2):\n",
    "    # Get entities\n",
    "    entity1, entity2 = get_entity_nodes(tree, entities, e1, e2)\n",
    "    \n",
    "    # Get head and rel of each entity\n",
    "    parent1, rel1 = get_entity_parent(tree, entity1)\n",
    "    parent2, rel2 = get_entity_parent(tree, entity2)\n",
    "    \n",
    "    # Features\n",
    "    features = ['h1_lemma=%s' % parent1['lemma'],\n",
    "                'h1_word=%s' % parent1['word'],\n",
    "                'h1_tag=%s' % parent1['tag'],\n",
    "                'h1_rel=%s' % rel1,\n",
    "                'h2_lemma=%s' % parent2['lemma'],\n",
    "                'h2_word=%s' % parent2['word'],\n",
    "                'h2_tag=%s' % parent2['tag'],\n",
    "                'h2_rel=%s' % rel2,\n",
    "                ]\n",
    "    \n",
    "    if same(parent1, parent2):\n",
    "        features.append('under_same')\n",
    "        if parent1['tag'][0].lower() == 'v':\n",
    "            features.append('under_same_verb')\n",
    "    \n",
    "    if is_under(entity1, entity2):\n",
    "        features.append('1under2')\n",
    "    \n",
    "    if is_under(entity2, entity1):\n",
    "        features.append('2under1')\n",
    "        \n",
    "    if parent_lemma_belongs(parent1, ['interact', 'interaction']):\n",
    "        features.append('interaction')\n",
    "    \n",
    "    start1, end1 = get_entity_limits(entity1)\n",
    "    start2, end2 = get_entity_limits(entity2)\n",
    "    for k in sorted(tree.keys()):\n",
    "        if 'start' in tree[k].keys() and  tree[k]['start'] < start1:\n",
    "            features.append('lb1=%s' % tree[k]['lemma'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end1 < tree[k]['start'] < start2:\n",
    "            features.append('lib=%s' % tree[k]['lemma'])\n",
    "\n",
    "        if 'start' in tree[k].keys() and end2 < tree[k]['start']:\n",
    "            features.append('la2=%s' % tree[k]['lemma'])\n",
    "      \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the subsidiary functions used in the **extract_features** function are the same as the ones used in **check_interaction** function of the Goal 1 and are explained there (**get_entity_nodes**, **get_entity_parent**, **is_under**, **same**, **parent_lemma_belongs**). \n",
    "\n",
    "The only other function used is the one below, **get_entity_limits(entity)** which receives a list of the nodes forming the entity and returns the start and end position inside the sentence. We consider the start of the entity as the start position of its first token and the end as the end position of its last token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entity_limits(entity):\n",
    "    start = min([e['start'] for e in entity])\n",
    "    end = max([e['end'] for e in entity])\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learner\n",
    "As we have said before, our learner is the **megam** seen in class. As it is a binary file in our **train** function we only call the **megam** implementation with a system command, with the corresponding parameters (-quiet -nc -repeat 5 -tune -lambda 0.01 -minfc 3), the model type (multiclass) and the training features obtained with the **extract_features** function and the Train dataset. The model is saved in the *model.dat* file which then will be used for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Learner\n",
    "As we have said before, our learner is the **megam** seen in class. As it is a binary file in our **train** function we only call the **megam** implementation with a system command, with the corresponding parameters (-quiet -nc -repeat 5 -tune -lambda 0.01 -minfc 3), the model type (multiclass) and the training features obtained with the **extract_features** function and the Train dataset. The model is saved in the *model.dat* file which then will be used for classification.def train(megam, parameters, features_file, out_train_model):\n",
    "    print(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" multiclass \" + features_file + \" > \" + out_train_model + \"\\\"\")\n",
    "    os.system(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" multiclass \" + features_file + \" > \" + out_train_model + \"\\\"\")### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = '-quiet -nc -repeat 5 -tune -lambda 0.01 -minfc 3'\n",
    "train(model_path+'megam.opt', parameters, features_path+'train_features', model_path+'model.dat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier\n",
    "In a similar way that what we did with the learner, to classify the interactions of our datasets we use a simple system command. This command calls megam. However, in prediction is not necessary to specify all the parameters used in training. We only use the '-quiet' and '-nc' options, and the '-prediction' to indicate to megam that we want to predict and not train again. We store the prediction obtained from megan to finally evaluate the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(megam, parameters, features_file, train_model, prediction_output):\n",
    "    print(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" -predict \" + train_model + \" multiclass \" + features_file + \" > \" + prediction_output + \"\\\"\")\n",
    "    os.system(\"ubuntu run \\\"\"+ megam + \" \" + parameters + \" -predict \" + train_model + \" multiclass \" + features_file + \" > \" + prediction_output + \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Devel prediction\n",
    "classify(model_path+'megam.opt', '-quiet -nc ', features_path+'devel_features', model_path+'model.dat', output_path+'devel_prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prediction\n",
    "classify(model_path+'megam.opt', '-quiet -nc ', features_path+'test_features', model_path+'model.dat', output_path+'test_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "The final results obtained for devel and test are shown below:\n",
    "\n",
    "#### Devel\n",
    "```\n",
    "SCORES FOR THE GROUP: develGoal RUN=1\n",
    "Gold Dataset: /Devel\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "260     99      224     484     0,7242  0,5372  0,6168\n",
    "\n",
    "\n",
    "Detection and classification of DDI\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "240     119     244     484     0,6685  0,4959  0,5694\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "86      65      115     201     0,5695  0,4279  0,4886\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "99      37      63      162     0,7279  0,6111  0,6644\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "53      16      66      119     0,7681  0,4454  0,5638\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "2       1       0       2       0,6667  1       0,8\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES FOR DDI CLASSIFICATION:\n",
    "        P       R       F1\n",
    "        0,6831  0,6211  0,6506\n",
    "________________________________________________________________________\n",
    "```\n",
    "\n",
    "#### Test\n",
    "```\n",
    "SCORES FOR THE GROUP: testGoal RUN=1\n",
    "Gold Dataset: /Test-DDI\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "714     521     265     979     0,5781  0,7293  0,645\n",
    "\n",
    "\n",
    "Detection and classification of DDI\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "619     616     360     979     0,5012  0,6323  0,5592\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "214     213     88      302     0,5012  0,7086  0,5871\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "222     221     138     360     0,5011  0,6167  0,5529\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "146     156     75      221     0,4834  0,6606  0,5583\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "37      26      59      96      0,5873  0,3854  0,4654\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES FOR DDI CLASSIFICATION:\n",
    "        P       R       F1\n",
    "        0,5183  0,5928  0,553\n",
    "________________________________________________________________________\n",
    "```\n",
    "\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "The global results are:\n",
    "\n",
    "**Devel: precision 0.6831, recall 0.6211 and F1 0.6506**\n",
    "\n",
    "**Test: precision 0.5183, recall 0.5928 and F1 0.5530**\n",
    "\n",
    "We clearly see that the machine learning solution significantly outperforms the baseline. We can conclude that an advanced approach, that extract features to encode data and generalizes a complex model to learn a problem, has better performance than only having a simple set of rules.\n",
    "\n",
    "Moreover, even in this approach, we have been able to maintain a good global balance between precision and recall in both datasets. Also, we have been able to achieve a high score in the Test dataset, without having a big drop from the Devel one. This is because in our experiments we have always controlled the overfitting when training, comparing the Train and Devel scores and ensuring the generalization of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
