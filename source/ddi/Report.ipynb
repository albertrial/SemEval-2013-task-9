{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AHLT - Second delivery - Task 9.2 DDI\n",
    "**Albert Rial**   \n",
    "**Karen Lliguin**   \n",
    "\n",
    "This delivery consists of solving the task 9.2 of the SemEval-2013 challenge. The task concerns classifying drug-drug interactions between pairs of drugs. \n",
    "\n",
    "The dataset provided contains XML files with sentences with drugs entities appearing on it and the corresponding interaction. There are four general types: mechanism, int, effect and advise. The data is already splitted in three subsets: Train, Devel and Test.\n",
    "\n",
    "To complete this task different methods and resources have been used among them, the Stanford CoreNLP dependency parser. We have divide the task in different subgoals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 1: Rule-based DDI\n",
    "### Introduction\n",
    "\n",
    "First, simple heur√≠stic rules have been used to carry out the task. In this version only the information given by the Train dataset is used. The final goal is to achieve an overall F1 score of at least 0.15 on the Devel dataset.\n",
    "\n",
    "### Data exploration\n",
    "\n",
    "\n",
    "With the purpose of building significant rules (and features for the next goal), first we have done a data exploration over the Test dataset. The following aspects have been analyse for each type interaction:\n",
    "- The most commom words that appear in between the two drugs that interact (clue_words).\n",
    "- The most commom words that appear in each sentence containing drugs that interact (sentence_words).\n",
    "- The most commom words from the dependency tree of the sentence where entity1 is under entity (e1_under).\n",
    "- The most commom words from the dependency tree of the sentence where entity1 is under entity (e2_under).\n",
    "\n",
    "Besides this analysis, in order to have a more clear understading of each metric regarding on how one word is seen by all the different types, we have store the previous information so that we could search a word regarding any of the previous metrics and the information of how many times it appears for each type is shown. For intance, given the word \"effect\" and the metric \"e1_under\" the following information is obtained:\n",
    "```\n",
    "--------------------\n",
    "effect\n",
    "48\n",
    "1525\n",
    "--------------------\n",
    "mechanism\n",
    "16\n",
    "1118\n",
    "--------------------\n",
    "int\n",
    "0\n",
    "186\n",
    "--------------------\n",
    "advise\n",
    "2\n",
    "707\n",
    "--------------------\n",
    "none\n",
    "307\n",
    "21553\n",
    "```\n",
    "This inside information is used to build the rules.\n",
    "\n",
    "### Rules\n",
    "\n",
    "As a first approach, \n",
    "\n",
    "### Details\n",
    "\n",
    "The functions *analyse*, *check_interaction* and *get_entity* are presented.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(sent):\n",
    "    if len(sent)<= 0:\n",
    "        return None\n",
    "    \n",
    "    mytree, = my_parser.raw_parse(sent)\n",
    "    tree = mytree.nodes\n",
    "    ini_token = 0\n",
    "                   \n",
    "    # clean tree\n",
    "    info = ['address', 'head', 'lemma', 'rel', 'word', 'tag']\n",
    "    for k in range(len(tree)):\n",
    "        node = tree[k] \n",
    "        for key in list(node):\n",
    "            if key not in info:\n",
    "                del node[key]\n",
    "        \n",
    "        if k != 0:\n",
    "            # add offsets\n",
    "            ini_token = sent.find(node['word'] ,ini_token)\n",
    "            node['start'] = ini_token\n",
    "            ini_token += len(node['word'])\n",
    "            node['end'] = ini_token - 1\n",
    "            \n",
    "    return tree\n",
    "\n",
    "def get_entity_nodes(tree, entities, e1, e2):\n",
    "    entity1 = []\n",
    "    entity2 = []\n",
    "    starts1 = entities[e1][0].split(';')\n",
    "    starts2 = entities[e2][0].split(';')\n",
    "    ends1 = entities[e1][1].split(';')\n",
    "    ends2 = entities[e2][1].split(';')\n",
    "    for k in tree.keys():\n",
    "        if 'start' in tree[k].keys():\n",
    "            if str(tree[k]['start']) in starts1 or str(tree[k]['end']) in ends1:\n",
    "                entity1.append(tree[k])\n",
    "            elif str(tree[k]['start']) in starts2 or str(tree[k]['end']) in ends2:\n",
    "                entity2.append(tree[k])\n",
    "    return entity1, entity2\n",
    "\n",
    "def check_interaction(analysis, entities, e1, e2):\n",
    "    # Get entities\n",
    "    entity1, entity2 = get_entity_nodes(analysis, entities, e1, e2)\n",
    "    \n",
    "    int_flag = 0\n",
    "    effect_flag = 0\n",
    "    mechanism_flag = 0\n",
    "    advise_flag = 0\n",
    "    for key in analysis.keys():\n",
    "        if 'start' in analysis[key].keys() and analysis[key]['word'] in [\"administration\", 'None','inhibitor']:\n",
    "            int_flag = 1\n",
    "        if 'start' in analysis[key].keys() and analysis[key]['word'] in ['drug','administer', 'effect', 'use','dose']:\n",
    "            effect_flag = 1\n",
    "        if 'start' in analysis[key].keys() and analysis[key]['word'] in ['drug', 'administer', 'dose','use','effect','concentration']:\n",
    "            mechanism_flag = 1\n",
    "        if 'start' in analysis[key].keys() and analysis[key]['word'] in ['drug','use','effect']:\n",
    "            advise_flag = 1\n",
    "            \n",
    "    if len(entity1) > 0 and len(entity2) > 0:\n",
    "        # DDI rules\n",
    "        # e1_e2_under_same_verb -> \"advise\"\n",
    "        # e1_e2_under_same_word_but_not_noun_or_verb -> none\n",
    "        # e1_under_e2 -> none\n",
    "        for ent1 in entity1:\n",
    "            if analysis[ent1['head']] in entity2:\n",
    "                return (0, \"null\")\n",
    "        \n",
    "        for ent1 in entity1:\n",
    "            for ent2 in entity2:\n",
    "                if ent1['head'] == ent2['head'] and analysis[ent1['head']]['tag'].lower()[0] not in ['v', 'n']:\n",
    "                    return (0, \"null\")\n",
    "                \n",
    "                if ent1['head'] == ent2['head'] and analysis[ent1['head']]['tag'].lower()[0] == 'v':\n",
    "                    return (1, \"advise\")\n",
    "    \n",
    "        for e in entity1:\n",
    "            if analysis[e['head']]['lemma'] in ['response', 'diminish', 'enhance'] and not effect_flag:\n",
    "                return (1, \"effect\")\n",
    "            elif analysis[e['head']]['lemma'] in ['absorption', 'metabolism', 'presence']and not mechanism_flag:\n",
    "                return (1, \"mechanism\")\n",
    "            elif analysis[e['head']]['lemma'] in ['interact', 'interaction'] and not int_flag:\n",
    "                return (1, \"int\")\n",
    "            elif analysis[e['head']]['lemma'] in ['take', 'adjustment', 'avoid', 'recommend', 'contraindicate'] and not advise_flag:\n",
    "                return (1, \"advise\")\n",
    "            \n",
    "        for e in entity2:\n",
    "            if analysis[e['head']]['lemma'] in ['effect']:\n",
    "                return (1, \"effect\")\n",
    "            elif analysis[e['head']]['lemma'] in ['absorption', 'metabolism', 'level', 'clearance']:\n",
    "                return (1, \"mechanism\")\n",
    "            elif analysis[e['head']]['lemma'] in ['take', 'caution']:\n",
    "                return (1, \"advise\")\n",
    "        \n",
    "        return (0, \"null\")    \n",
    "    else:\n",
    "        return (0, \"null\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "#### Devel\n",
    "```\n",
    "Gold Dataset: /Devel\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "135     227     349     484     0,3729  0,2789  0,3191\n",
    "\n",
    "\n",
    "Detection and classification of DDI\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "122     240     362     484     0,337   0,2521  0,2884\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "60      134     141     201     0,3093  0,2985  0,3038\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "43      52      119     162     0,4526  0,2654  0,3346\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "17      32      102     119     0,3469  0,1429  0,2024\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "2       22      0       2       0,0833  1       0,1538\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES FOR DDI CLASSIFICATION:\n",
    "        P       R       F1\n",
    "        0,298   0,4267  0,351\n",
    "________________________________________________________________________\n",
    "```\n",
    "#### Test\n",
    "```\n",
    "Gold Dataset: /Test-DDI\n",
    "\n",
    "Partial Evaluation: only detection of DDI (regadless to the type)\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "260     452     719     979     0,3652  0,2656  0,3075\n",
    "\n",
    "\n",
    "Detection and classification of DDI\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "214     498     765     979     0,3006  0,2186  0,2531\n",
    "\n",
    "\n",
    "________________________________________________________________________\n",
    "\n",
    "SCORES FOR DDI TYPE\n",
    "Scores for ddi with type mechanism\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "88      182     214     302     0,3259  0,2914  0,3077\n",
    "\n",
    "\n",
    "Scores for ddi with type effect\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "48      53      312     360     0,4752  0,1333  0,2082\n",
    "\n",
    "\n",
    "Scores for ddi with type advise\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "32      121     189     221     0,2092  0,1448  0,1711\n",
    "\n",
    "\n",
    "Scores for ddi with type int\n",
    "tp      fp      fn      total   prec    recall  F1\n",
    "46      142     50      96      0,2447  0,4792  0,3239\n",
    "\n",
    "\n",
    "MACRO-AVERAGE MEASURES FOR DDI CLASSIFICATION:\n",
    "        P       R       F1\n",
    "        0,3138  0,2622  0,2857\n",
    "```\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "The final results obtained are:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2: ML-based DDI\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "#### Devel\n",
    "\n",
    "#### Test\n",
    "\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "The final results obtained are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
